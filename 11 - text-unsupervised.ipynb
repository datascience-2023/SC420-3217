{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9e28ad-fd37-4453-9e46-10df10c4ab0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Работа со сложными данными и обучение без учителя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a2237-2177-47be-a4ab-3153ac34a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "                                    LabelEncoder, OneHotEncoder, label_binarize, OrdinalEncoder, \n",
    "                                    StandardScaler, QuantileTransformer, PowerTransformer, MinMaxScaler, RobustScaler\n",
    "                                  )\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, roc_curve,\n",
    "                             precision_recall_curve, classification_report, recall_score, precision_score,\n",
    "                             log_loss, brier_score_loss)\n",
    "\n",
    "from category_encoders import TargetEncoder, WOEEncoder, HashingEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f762c-d36f-4ffd-a224-30850e56e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d67c9-edd1-48e9-95a9-9bb364579d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b46cf6-0712-4c37-a1b2-05e5ddd16963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6c841-4cd2-4383-a50f-4770287d215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 177013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214172c-7168-4eb6-a1ad-711e658066ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Exited'], axis=1),\n",
    "                                                    df['Exited'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=177013,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=df['Exited']\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dafaa-c023-4bf6-996b-8102400afa19",
   "metadata": {},
   "source": [
    "## Кодирование категорий"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825154c-a106-4157-9673-ccc3de776fea",
   "metadata": {},
   "source": [
    "### One-Hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9211b0e-f214-4847-b2cb-8b6fa85a8daf",
   "metadata": {},
   "source": [
    "Простой и универсальный метод: каждое возможное значение можно описать бинарным признаком. Для N значений достаточно N-1 таких признаков.\n",
    "\n",
    "Для всего датасета через pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200b222-455d-48ad-9426-d1fb88ed1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train['Geography'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82f864-f75f-4772-9d08-715b33c53621",
   "metadata": {},
   "source": [
    "Если у вас отдельные сеты, метод из pandas ненадежен, поскольку не гарантируется, что везде присутствуют все возможные значения.\n",
    "\n",
    "Через sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7feb9e-af47-43fe-971c-42502e013ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False).set_output(transform='pandas')\n",
    "encoder.fit_transform(X_train[['Geography']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba04714-1aaf-42ac-87ca-f48901af2c29",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6a134-6793-4f0e-b30d-5027a79a88c6",
   "metadata": {},
   "source": [
    "Если порядок категорий имеет смысл, можно закодировать их числами по возрастанию. Деревянные модели неплохо понимают такое кодирование, а вот для прочих использовать его не стоит.\n",
    "\n",
    "Кодирование в алфавитном порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cadb2-0766-4097-8154-4b615e96bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "X_train['geo_encoded'] = encoder.fit_transform(X_train[['Geography']])\n",
    "X_train[['Geography', 'geo_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675711c8-a638-43a7-a60c-f0dbaa65a9bb",
   "metadata": {},
   "source": [
    "Кодирование в порядке появления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1384b-ae0f-42f5-800d-fd1b10cb3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['geo_encoded'], uniques = pd.factorize(X_train['Geography'])\n",
    "X_train[['Geography', 'geo_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93797534-4a11-47b4-b92e-0df4c5ffdf4e",
   "metadata": {},
   "source": [
    "В произвольном порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b787c17-d265-4860-bcc1-717d53327184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['geo_encoded'] = label_binarize(X_train['Geography'], classes=['Germany', 'France', 'Spain']).argmax(axis=1)\n",
    "X_train[['Geography', 'geo_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7b5b3-bcad-4c6b-af41-a0bb4c7b33f8",
   "metadata": {},
   "source": [
    "### Frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155aa59-cbb4-4ae5-9e8f-62e85634767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['geo_encoded'] = st.rankdata(X_train['Geography'], method='average')\n",
    "X_train[['Geography', 'geo_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a9278-5f09-4e6d-9b57-bffcaf17730b",
   "metadata": {},
   "source": [
    "### Weight of evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7685dab-0f96-49b6-a873-c9db0708b4ef",
   "metadata": {},
   "source": [
    "Предназначен для задачи бинарной классификации. По сути является логарифмом отношения долей положительного и отрицательного класса для данного значения категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc31270-5b61-4bba-b003-56c0430b164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = WOEEncoder()\n",
    "X_train['geo_encoded'] = encoder.fit_transform(X_train['Geography'], y_train)\n",
    "X_train[['Geography', 'geo_encoded']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bce10-7506-482b-9951-960ead7e29dd",
   "metadata": {},
   "source": [
    "**Этот метод использует целевой признак! Фитить его следует только на обучающей выборке после разбиения!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc528033-fb38-42a4-abb2-cf399c67310f",
   "metadata": {},
   "source": [
    "### Mean-target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76a58b-d6e0-448a-a8d9-a44ddcf16e10",
   "metadata": {},
   "source": [
    "Заменяет категорию на среднее значение целевого признака по этой категории. Хорошо подходит для случаев большой кардинальности, однако следует быть осторожным в плане утечки целевого признака для очень редких категорий. В `category_encoders` есть более робастные реализации, например, `LeaveOneOutEncoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef7a6f-03ce-4366-a161-4f3f73a56166",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TargetEncoder()\n",
    "X_train['geo_encoded'] = encoder.fit_transform(X_train['Geography'], y_train)\n",
    "X_train[['Geography', 'geo_encoded']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dcde9-5785-4f81-ae07-85aaf3eccf10",
   "metadata": {},
   "source": [
    "**Этот метод использует целевой признак! Фитить его следует только на обучающей выборке после разбиения!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b7299-0138-4e00-8c51-7fa425b6abdb",
   "metadata": {},
   "source": [
    "### Хеширование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c9278-9dad-40e4-b2f1-fde0a4f0bfa9",
   "metadata": {},
   "source": [
    "Этот метод подходит, если категорий много, но некоторая потеря информации допустима."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c5a59-2ea9-469b-a2ea-0c79bd40f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HashingEncoder()\n",
    "encoder.fit_transform(X_train['Geography']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e328e6-e961-4226-9979-d2a9e25e95c1",
   "metadata": {},
   "source": [
    "Более подробную блок-схему выбора метода для кодировки можно найти на https://innovation.alteryx.com/encode-smarter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb33558-f051-4fd8-9011-8503455b2ad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Кодирование внутри конвейера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68f63a-f135-405a-90f0-6225a562bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Geography', 'Gender']\n",
    "useless_columns = ['CustomerId', 'RowNumber', 'Surname', 'geo_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bacc8-2293-4c0b-96ec-9ee09e9ea43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка по группам признаков:\n",
    "transformers = [\n",
    "                    (\"encoder\", OneHotEncoder(drop='first'), cat_columns),\n",
    "                    (\"drop\", \"drop\", useless_columns),\n",
    "               ]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8964ad-7403-4064-8c69-332ab1815060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, params, X, y):\n",
    "    name = f'{type(model).__name__}'\n",
    "    print(f'Оптимизация {name}...')\n",
    "    pipe = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('scaler', None),                \n",
    "                        ('model', model)\n",
    "                    ])\n",
    "    gcv = GridSearchCV(pipe, params, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gcv.fit(X, y)\n",
    "    print(f'Лучшие гиперпараметры: {dict(gcv.best_params_)}')\n",
    "    print(f'Скор: {(-gcv.best_score_):.2f}')\n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d609367-88c2-4168-9dff-fe7313094746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26f004-2a1a-440b-8a2a-dfe11f54e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_params = {\n",
    "                    'model__max_depth' : range(1, 10),\n",
    "                    'model__max_iter' : [200, 500, 1000],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71bd90-6b61-45c1-90e1-e082e82ed303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hgb = optimize(HistGradientBoostingClassifier(random_state=RANDOM_STATE), boost_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78cb05-f145-47f1-85cf-884d26d3ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(target_test, probabilities):\n",
    "    cmatrix = confusion_matrix(target_test, probabilities > 0.5)\n",
    "\n",
    "    ap = average_precision_score(target_test, probabilities)\n",
    "    fpr, tpr, _ = roc_curve(target_test, probabilities)\n",
    "    roc_auc = roc_auc_score(target_test, probabilities)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(target_test, probabilities)\n",
    "    f_scores = 2 * recall * precision / (recall + precision)\n",
    "    f_scores = np.nan_to_num(f_scores)\n",
    "    best_thresh = thresholds[np.argmax(f_scores)]\n",
    "    best_f = np.max(f_scores)\n",
    "    best_acc = accuracy_score(target_test, (probabilities > best_thresh))\n",
    "    best_cmatrix = confusion_matrix(target_test, (probabilities > best_thresh))\n",
    "\n",
    "    return best_f, roc_auc, best_acc, ap, best_thresh, fpr, tpr, recall, precision, cmatrix, best_cmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50757a-7c20-428b-85bf-2bac0f35108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(target_test, probabilities):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "    axes[0].plot([0, 1], linestyle='--')\n",
    "    axes[1].plot([0.5, 0.5], linestyle='--')\n",
    "\n",
    "    best_f, roc_auc, acc, ap, best_thresh, fpr, tpr, recall, precision, cmatrix, best_cmatrix = calculate_metrics(target_test, probabilities)\n",
    "    print(f'ROC_AUC: {roc_auc:.2f}, AP (PR_AUC): {ap:.2f}, наилучший F1: {best_f:.2f} с порогом {best_thresh:.2f} (accuracy {acc:.2f})')\n",
    "    axes[0].plot (fpr, tpr);\n",
    "    axes[1].plot (recall, precision);\n",
    "\n",
    "    axes[0].set (xlabel='FPR', ylabel='TPR', title='ROC-кривая', xlim=(0,1), ylim=(0,1))\n",
    "    axes[1].set (xlabel='Recall', ylabel='Precision', title='PR-кривая', xlim=(0,1), ylim=(0,1))\n",
    "    plt.show()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "    sns.heatmap(cmatrix, ax=axes[0], annot=True, cmap='Blues', fmt='d').set(title='Матрица ошибок', xlabel='Предсказание', ylabel='Реальность')\n",
    "    sns.heatmap(best_cmatrix, ax=axes[1], annot=True, cmap='Blues', fmt='d').set(title='Матрица ошибок (оптимальный порог)', xlabel='Предсказание', ylabel='Реальность')\n",
    "\n",
    "    \n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529a084-1758-41ad-9970-72995ed95595",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(y_test, best_hgb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f95e3-89b6-4ec2-822f-78230f80550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = [\n",
    "                OneHotEncoder(drop='first'),\n",
    "                OrdinalEncoder(),\n",
    "                WOEEncoder(),\n",
    "                TargetEncoder(),\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b39653-f2f7-4ac4-96a4-5fd61730b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_params = {\n",
    "                    'preprocessor__encoder' : encoder_list, \n",
    "                    'model__max_depth' : range(1, 10),\n",
    "                    'model__max_iter' : [200, 500, 1000],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5d3ec-b1bf-4c62-9139-3a03360029c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hgb = optimize(HistGradientBoostingClassifier(random_state=RANDOM_STATE), boost_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5053c73-d4a9-45cd-8b0f-1e9e1d507b41",
   "metadata": {},
   "source": [
    "## Простое кодирование текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1491c-6bff-41da-be3d-194928fbb67b",
   "metadata": {},
   "source": [
    "Воспользуемся датасетом твитов о стихийных бедствиях и катастрофах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef90cbc-ff28-4b6a-9e3c-6400c3d78d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a563a76-d9a6-4d89-a4e9-7dcc780373ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('disaster_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496dbc20-a425-46db-9cc0-df7d4df94c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bd63c-1afa-4f6e-9997-a7b22ca6ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3040d4-ce45-4def-8f37-d299fe2239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290c7fd-c67d-4d2c-bf1c-531bac3ddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = 'text'\n",
    "useless_columns = ['id', 'keyword', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f25ce-dfd9-402a-8360-0dc780d1492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'], axis=1),\n",
    "                                                    df['target'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=177013,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=df['target']\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15959978-bf8c-4b1e-8539-cfa6e5ad8280",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fd982-7eea-4c24-9473-7edb55af2abe",
   "metadata": {},
   "source": [
    "Самый простой способ - найти все уникальные слова и выделить бинарный признак на наличие каждого в тексте.\n",
    "\n",
    "Недостатки достаточно очевидные:\n",
    "\n",
    "- совершенно не учитывается порядок слов;\n",
    "- уникальных слов может быть много - придется урезать число новых признаков в пользу самых частых.\n",
    "\n",
    "Тем не менее, во многих случаях этот способ неплохо работает! В sklearn он представлен `sklearn.feature_extraction.text.CountVectorizer()`. При желании можно даже провести обратное преобразование и найти ключевые слова с помощью feature importance.\n",
    "\n",
    "Чуть более быстрый способ - `HashingVectorizer()`, у него обратного преобразования нет, но как небольшой плюс он масштабирует признаки на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955f646-9641-4f88-9477-5361d1234945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка по группам признаков:\n",
    "transformers = [\n",
    "                    (\"encoder\", CountVectorizer(max_features=500), text_columns),\n",
    "                    (\"drop\", \"drop\", useless_columns),\n",
    "               ]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough', n_jobs=-1, sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16251b62-93ee-40d2-8697-159b8e5e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, params, X, y):\n",
    "    name = f'{type(model).__name__}'\n",
    "    print(f'Оптимизация {name}...')\n",
    "    pipe = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('scaler', None),                \n",
    "                        ('model', model)\n",
    "                    ])\n",
    "    gcv = GridSearchCV(pipe, params, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gcv.fit(X, y)\n",
    "    print(f'Лучшие гиперпараметры: {dict(gcv.best_params_)}')\n",
    "    print(f'Скор: {(-gcv.best_score_):.2f}')\n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedff57-270f-4bf3-9a7f-71203e0deb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список скейлеров:\n",
    "scaler_list = [\n",
    "               StandardScaler(),\n",
    "               PowerTransformer(),\n",
    "               QuantileTransformer(random_state=RANDOM_STATE),\n",
    "               QuantileTransformer(random_state=RANDOM_STATE, output_distribution='normal'),\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb494d-f7a9-4ea8-8850-60314105f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = [\n",
    "                    CountVectorizer(max_features=500, stop_words='english'),\n",
    "                    #CountVectorizer(max_features=500, ngram_range=(1,2)),\n",
    "                    HashingVectorizer(n_features=500, stop_words='english'),\n",
    "                    TfidfVectorizer(max_features=500, stop_words='english'),\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2913108-cfb9-49a2-aef4-baf4f05fd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "                    #'preprocessor__encoder':encoder_list,\n",
    "                    'scaler':scaler_list,\n",
    "                    'model__C':np.logspace(-3, 3, 7),\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dee3ec-be86-4a15-92d7-24308e625b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = optimize(LogisticRegression(solver='newton-cholesky', random_state=RANDOM_STATE, n_jobs=-1), lr_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c5da7-ef58-4ccc-bc8b-55ca7616ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(y_test, best_lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee127d-3722-4437-b7f9-cfae2107f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi_importances = pd.Series(best_lr['model'].coef_[0], index=best_lr[:-1].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d28d1-f9c3-47ed-979c-dfd5e795aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdi_importances.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d2275-f67a-4f6e-b8d6-00d248bb7524",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6613a8f-398a-405e-a531-f3eb0947e93f",
   "metadata": {},
   "source": [
    "Можно строить признаки не только на отдельных словах, но и на словах, встречающихся в паре (или в более длинной последовательности). Для этого достаточно воспользоваться параметром `ngram_range`, например вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d768814-bcea-402e-a7cb-5342bf7780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer(max_features=200, ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db86cbf-6884-4deb-a293-2a7ce9f69bd1",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d994a8-d8ba-4ce8-aa24-fdae4b91e0e2",
   "metadata": {},
   "source": [
    "Этот способ учитывает употребление слова как в конкретной записи, так и во всем столбце.\n",
    "\n",
    "Term Frequency:\n",
    "\n",
    "$$\n",
    "TF = \\frac{t}{n}\n",
    "$$\n",
    "\n",
    "$t$ - число повторов слова в тексте, $n$ - длина текста\n",
    "\n",
    "Inverse Document Frequency:\n",
    "\n",
    "$$\n",
    "IDF = log_{10}\\frac{D}{d}\n",
    "$$\n",
    "\n",
    "$D$ - общее число текстов, $d$ - число текстов, где слово встречается.\n",
    "\n",
    "Реализован в `TfidfVectorizer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee7161-ea94-45b6-ad18-929cb4d71024",
   "metadata": {},
   "source": [
    "### Самостоятельная работа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d26e3d-5cf6-45f2-a2a3-303e50a42d76",
   "metadata": {},
   "source": [
    "Измените гиперпараметры выше, чтобы обучить модель на биграмах (можно использовать любой векторайзер). Стало ли лучше? Как думаете, почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf4b79-57d8-459b-a1f9-bc6248e387e7",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b8a82-bd62-4937-bb9d-e78c44437da5",
   "metadata": {},
   "source": [
    "Для классических методов нам желательно не включать ничего лишнего в данные, чтобы получить рабочее и компактное преобразование. В этом могут помочь:\n",
    "\n",
    "- удаление пунктуации;\n",
    "- чистка от специфического мусора (гиперссылки, эмодзи и т. п.);\n",
    "- задание списка стоп-слов, которые встречаются часто, но мало добавляют к смыслу (артиклей, предлогов, местоимений...);\n",
    "- лемматизация (приведение слова к словарной форме);\n",
    "- стемминг (выделение основы слова).\n",
    "\n",
    "Основные библиотеки для исследования и обработки текстов:\n",
    "\n",
    "- NLTK\n",
    "- spaCy\n",
    "- gensim\n",
    "- TextBlob\n",
    "\n",
    "На примере библиотеки gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7037da-f5ad-47a3-8630-8419cb0383b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = X_train['text'].iloc[9]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c41c9e-c524-4b20-ab5a-0ab8500c0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_non_alphanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65194190-7df7-47b0-9677-cf9a04c3e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_non_alphanum(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e904392-6197-4c70-9012-c1a7e21bebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83388a5c-559e-49d1-9ffc-d81d0a8b618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "p.stem_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb55e3-4528-4656-83bf-c80c0309f744",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a9e5b7-241e-4759-b2ee-21f02686e4ef",
   "metadata": {},
   "source": [
    "Наконец, для обработки предложение надо разбить на список слов (векторайзеры из sklearn делают это сами, но им можно указать свои функции для предобработки и токенизации). В большинстве случаев достаточно `split()`, но если надо, например, сохранить пунктуацию, процесс усложняется.\n",
    "\n",
    "`preprocess_string()` из gensim - один из вариантов пакетной обработки, от чистки до стемминга и токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da845e5-7647-4182-8d15-1f889a098315",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_string(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3d7b0-1458-4549-8283-75ed668e67ce",
   "metadata": {},
   "source": [
    "Библиотеки, предоставляющие лемматизацию: https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c95f9-bf90-405a-8b03-46c6b8f1eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    return ' '.join(preprocess_string(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906d61e-18b6-4bb8-ae9b-329b73db3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "                    'preprocessor__encoder':[CountVectorizer(max_features=500, preprocessor=clean)],\n",
    "                    'scaler':scaler_list,\n",
    "                    'model__C':np.logspace(-3, 3, 7),\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ab36e-1b56-4f2d-be0f-8e9d3e6c6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = optimize(LogisticRegression(solver='newton-cholesky', random_state=RANDOM_STATE, n_jobs=-1), lr_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01986b1a-89c6-4173-b302-2246b516f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(y_test, best_lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cb753-9522-4f84-90af-536f72673dab",
   "metadata": {},
   "source": [
    "## Word2Vec (на примере gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6dd47-74b4-486c-b003-4589fd309054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce655c3-bf2e-491f-a7e0-69883f6ccec4",
   "metadata": {},
   "source": [
    "Можно обучить Word2Vec на наших данных, но результат будет так себе. Для качественных эмбеддингов требуется большой корпус данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d05b7-8d0c-4088-8f7e-2fbb53b65d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_trained = Word2Vec(\n",
    "                                X_train['text'].apply(preprocess_string),\n",
    "                                vector_size=100,\n",
    "                                min_count=25,\n",
    "                                window=25,\n",
    "                                workers=-1,\n",
    "                             ).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2c06b-1a7d-43d3-bc9c-e0b8ac8aee61",
   "metadata": {},
   "source": [
    "К счастью, предобученных моделей много в свободном доступе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034f99a-f383-46df-8426-b719a7b58173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aba95a-557a-442b-9064-510f9232be4e",
   "metadata": {},
   "source": [
    "### Свойства векторов эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e9282-9ff5-4291-87d8-a7399ab3a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, similarity in model_glove_twitter.similar_by_word('cat')[:5]:\n",
    "    print(f\"{key}: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30db02a-a839-4a5f-90d1-fa4ca87797a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector = model_glove_twitter['kitten'] - model_glove_twitter['cat'] +  model_glove_twitter['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83782e8d-d1bf-4d46-ab8a-5fc2c33511d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, similarity in model_glove_twitter.similar_by_vector(new_vector)[:1]:\n",
    "    print(f\"{key}: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6f25d-217f-4857-85e2-4944eab05f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.most_similar(positive=['kitten', 'dog'], negative=['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c3e60-5cce-4a7f-9cef-c3693b376bbb",
   "metadata": {},
   "source": [
    "### Совместимость предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834aafec-e240-4d5a-b276-2ad8a70a72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_glove_twitter.key_to_index)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980c4ca-82e3-478b-b7bf-b0e88aaa04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dac673-9e54-4144-bb5a-f2aff244ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hashtag(hashtag):\n",
    "    hashtag_body = hashtag[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<HASHTAG> {} <ALLCAPS>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \"<HASHTAG> {}\".format(\" \".join(re.findall(r'[A-Z][^A-Z]*', hashtag_body)))\n",
    "\n",
    "    return result\n",
    "def preprocess_twitter(input):\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    input = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<URL>\", input)\n",
    "    input = re.sub(\"/\", \" / \", input)\n",
    "    input = re.sub(r\"@\\w+\", \"<USER>\", input)\n",
    "    input = re.sub(r\"{0}{1}[)d]+|[)d]+{1}{0}\".format(eyes, nose), \"<SMILE>\", input, flags=re.IGNORECASE)\n",
    "    input = re.sub(r\"{0}{1}p+\".format(eyes, nose), \"<LOLFACE>\", input, flags=re.IGNORECASE)\n",
    "    input = re.sub(r\"{0}{1}\\(+|\\)+{1}{0}\".format(eyes, nose), \"<SADFACE>\", input)\n",
    "    input = re.sub(r\"{0}{1}[\\/|l*]\".format(eyes, nose), \"<NEUTRALFACE>\", input)\n",
    "    input = re.sub(\"<3\", \"<HEART>\", input)\n",
    "    input = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", input)\n",
    "    input = re.sub(r\"#\\S+\", lambda hashtag: process_hashtag(hashtag.group(0)), input)\n",
    "    input = re.sub(r\"([!?.]){2,}\", lambda match: match.group(1) + \" <REPEAT>\", input)\n",
    "    input = re.sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", lambda match: match.group(1) + match.group(2) + \" <ELONG>\", input)\n",
    "    #input = re.sub(r\"\\b[A-Z][A-Z]+\\b\", lambda word: word.group(0).lower() + \" <ALLCAPS>\", input)\n",
    "    #input = re.sub(r\"([^a-z0-9()<>'`\\-]){2,}\", lambda word: word.group(0).lower() + \" <ALLCAPS>\", input)\n",
    "\n",
    "    return re.sub(r\"[^a-z <>]\",' ', input.lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c61593-e262-4822-a56b-cc1970175db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c86da-aa1d-4aef-b749-ff47aab469ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_twitter(X_train['text'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055b2f0-6c0a-46c5-ad1b-cd7fb8e497a4",
   "metadata": {},
   "source": [
    "### Как построить эмбеддинг для фразы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c125f-0206-4856-aecc-a5ef0dedb0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence, embeddings=model_glove_twitter, dim=100):\n",
    "    result = []\n",
    "    token_list = preprocess_twitter(sentence)\n",
    "    for token in token_list:\n",
    "        if token in embeddings:\n",
    "            result.append(embeddings[token])\n",
    "    return np.mean(result, axis=0) if result else np.zeros(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e5200-ca52-4c58-a5c8-7e38eded3fe3",
   "metadata": {},
   "source": [
    "### Обучение классификатора на эмбеддингах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce920c-bf7e-4d89-84c7-8b7a0c771fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = np.array([sentence_to_vec(x) for x in X_train['text'].values])\n",
    "X_test_embed = np.array([sentence_to_vec(x) for x in X_test['text'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1c817-da8a-404b-81c0-8025e3749ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "                    'preprocessor':[None],\n",
    "                    'scaler':scaler_list,\n",
    "                    'model__C':np.logspace(-3, 3, 7),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e522ae-2a94-4741-bbf2-48a2764a4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = optimize(LogisticRegression(solver='newton-cholesky', random_state=RANDOM_STATE, n_jobs=-1), lr_params, X_train_embed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee167415-6990-428a-9663-cee4b2e440c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(y_test, best_lr.predict_proba(X_test_embed)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db47cb-441f-4d27-82f1-b91ae9c69c17",
   "metadata": {},
   "source": [
    "## Проекция на меньшую размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe000c-6573-49bd-94e0-64642b381e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = ['Noto Sans CJK JP', 'sans-serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429e05f-dfd8-460b-98f4-b2421f9225fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c1b01-7df7-4b60-b67b-ad6de030f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_w2v(model, w2v=None, save=None):\n",
    "    if save:\n",
    "        fig, ax = plt.subplots(figsize=(128,128))\n",
    "    plt.scatter(model[:, 0], model[:, 1], s=1);\n",
    "    if save:\n",
    "        for i, v in enumerate(w2v.vectors[:10000]):\n",
    "            word = w2v.index_to_key[i]\n",
    "            plt.annotate(word, (model[i, 0], model[i, 1]))\n",
    "        plt.savefig(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6835c75-615b-4986-be2a-057b3e6015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(model_glove_twitter.vectors[:10000])\n",
    "plot_w2v(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d039b23-973a-43a7-bfbf-fdd1272691d2",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6318f9-4a9f-4fce-b297-6c0625cd9633",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10062dc1-62e0-489d-b2dc-29bb41532225",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, metric='cosine', n_jobs=-1, random_state=RANDOM_STATE).fit_transform(model_glove_twitter.vectors[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a5c4-783f-444e-bc75-6b6604f92d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_w2v(tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f310f8-02ed-4396-9d37-43dcd89918e4",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321400b9-ae2f-4409-bbb2-d64f6159a191",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce97b3-3402-436e-b2af-8c842171aade",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap = UMAP(n_components=2, metric='cosine', n_neighbors=5, random_state=RANDOM_STATE).fit_transform(model_glove_twitter.vectors[:10000])\n",
    "plot_w2v(umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131165f1-f70f-4958-af8a-b921a4fa8dfb",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95a7ef-abf1-4ecc-a202-c8dd65c7b382",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34398fb3-6b00-4610-9c16-5123705590e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skfuzzy.cluster import cmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82901db8-4c2b-41a6-9721-f96ce8ea56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def elbow_plot(data, max_clusters=20):\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(1, max_clusters+1):\n",
    "        result = cmeans(data, c=i, m=5.0, error=5e-3, maxiter=1000, seed=177013)\n",
    "        metrics.append(result[4][-1])\n",
    "    \n",
    "    plt.plot(range(1, max_clusters+1), metrics)\n",
    "    plt.title('График локтя')\n",
    "    plt.xlabel('Число кластеров')\n",
    "    plt.ylabel('Среднее расстояние до центра')\n",
    "    plt.xlim((1, max_clusters))\n",
    "    plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0840a0-9d5c-4a4f-9332-d6978600c560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elbow_plot(umap.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5f3f7-2d2a-48a8-af4e-78976e0ec862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skfuzzy.cluster import cmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9bf54-ba69-4ca3-8dce-b712cfcc4d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = cmeans(umap.T, c=10, m=2.0, error=5e-3, maxiter=1000, seed=177013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557b6ae-5a9b-4b83-9073-f007cf996fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(result[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91706f08-fa5a-4ff1-bd85-c7897e6cb6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centers = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034898e-1dd0-4b3b-af1e-e41605da8b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(umap[:, 0], umap[:, 1], c=preds, cmap='icefire');\n",
    "plt.scatter(centers[:, 0], centers[:,1], marker='+', s=400, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f77f5-7416-4a54-a205-95eccb69ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bfcec-ed74-4fa6-9e89-3a9d67cf8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=10, random_state=RANDOM_STATE, n_init='auto')\n",
    "kmeans = model.fit_predict(umap)\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c=kmeans, cmap='icefire');\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], marker='+', s=400, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f29725-cd1f-4050-b636-72c97f140f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_plot(data, method, max_clusters=20):\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(1, max_clusters+1):\n",
    "        model = method(n_clusters=i, random_state=RANDOM_STATE, n_init='auto')\n",
    "        model.fit(data)\n",
    "        metrics.append(model.inertia_)\n",
    "    \n",
    "    plt.plot(range(1, max_clusters+1), metrics)\n",
    "    plt.title('График локтя')\n",
    "    plt.xlabel('Число кластеров')\n",
    "    plt.ylabel('Среднее расстояние до центра')\n",
    "    plt.xlim((1, max_clusters))\n",
    "    plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f8d24-57a8-4ab1-a3f0-15c9c7b7b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_plot(umap, KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ab02f-9a64-4c43-a155-fcb55e6352dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5, random_state=RANDOM_STATE, n_init='auto')\n",
    "kmeans = model.fit_predict(umap)\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c=kmeans, cmap='icefire');\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], marker='+', s=400, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007c27b-95bf-483d-9a45-2b2905a790aa",
   "metadata": {},
   "source": [
    "### Иерархическая кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9b9f8-0532-4689-965b-ecd71c022669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a95df3-0587-4d0f-8a60-3e12a69eadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier = AgglomerativeClustering(n_clusters=5)\n",
    "agg_result = hier.fit_predict(umap)\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c=agg_result, cmap='icefire');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54103cb7-ccfa-4166-b64a-220c9ef05a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "#dendrogram(linkage(umap, method='ward'), truncate_mode='level');\n",
    "plt.title('Дендрограмма')\n",
    "plt.ylabel('Расстояние');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3b1d0-ad95-424f-909d-03253f7314ff",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215d30b-b024-4285-ba4e-4414a96c2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac7124-e74f-438d-9cfb-ed446dfee29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1, min_samples=5, n_jobs=-1)\n",
    "dbs = dbscan.fit_predict(umap)\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c=dbs, cmap='icefire');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905939e-6265-4fcb-a9e9-6c11515ad1c5",
   "metadata": {},
   "source": [
    "## Выбросы и аномалии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad50f8-94b3-40fa-9ebc-5d3ebc32c257",
   "metadata": {},
   "source": [
    "Мы уже учились выявлять выбросы в рамках отдельных признаков с помощью диаграммы размаха. Здесь мы разберем некоторые продвинутые методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b573a5-09b4-4106-b1ba-fd20ca4dd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('framingham_heart_disease.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6fa667-31f2-4a61-a463-e72be1147312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c95aa-eae9-41f4-9156-e3dc7e67a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd2fa3-5494-499f-9fc6-2aa19e0b86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adb8bc-28ca-4b4d-b232-d984b93ea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d81dc4-c9b9-4f58-b79f-acec266c43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = df.groupby('TenYearCHD')['glucose'].hist(bins='fd', figsize=(10,4), alpha=0.5);\n",
    "ax.set(xlabel='Уровень глюкозы', ylabel='Наблюдений, шт', title='Распределение уровня глюкозы в крови')\n",
    "ax.legend(['Здоровые', 'Больные']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515053c1-6319-4360-b47c-8d1a91f6cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = df.groupby('TenYearCHD')['BMI'].hist(bins='fd', figsize=(10,4), alpha=0.5);\n",
    "ax.set(xlabel='ИМТ', ylabel='Наблюдений, шт', title='Распределение ИМТ')\n",
    "ax.legend(['Здоровые', 'Больные']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c332a3f-23f3-401e-8b17-cdc1bc657117",
   "metadata": {},
   "source": [
    "### Применение функций расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810596f-50a8-4a7e-8e8c-6b593a38c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_space = ['BMI', 'glucose']\n",
    "\n",
    "# Матрица ковариации:\n",
    "covariance = np.cov(df[outlier_space], rowvar=False)\n",
    "inv_covariance = np.linalg.inv(covariance)\n",
    "\n",
    "# Центр:\n",
    "center = np.mean(df[outlier_space], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd03594-881d-407c-ba48-988d68c375d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def calc_distance(row):\n",
    "    return mahalanobis(row, center, inv_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd845975-1302-40a6-9927-391f6b348391",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = df[outlier_space].apply(calc_distance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7c4b6-540c-4874-aeec-1abf8941479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отсекаем 1% выбросов:\n",
    "limit = distance.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904b201-5054-4709-95d5-5eaefd655801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1800a-8520-47b4-bf0a-e132c96f38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_, v = np.linalg.eig(covariance)\n",
    "lambda_ = np.sqrt(lambda_)\n",
    "ellipse = Ellipse(xy=(center[0], center[1]),\n",
    "                  width=lambda_[0] * limit * 2, height=lambda_[1] * limit * 2,\n",
    "                  angle=np.rad2deg(np.arccos(v[0, 0])))\n",
    "ellipse.set_alpha(0.5)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = plt.subplot()\n",
    "plt.scatter(x=df['BMI'], y=df['glucose']);\n",
    "ax.add_artist(ellipse)\n",
    "ax.set(xlim=(0), ylim=(0), title='Границы выбросов', xlabel='BMI', ylabel='Glucose');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccf98f-cd7e-418c-9db2-3e89a98831ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this out to test without outlier exclusion:\n",
    "df[distance < limit][outlier_space].agg(['min', 'max']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea5f48-9352-4fd7-ac60-1ebefc1ea461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['BMI'] = df['BMI'].apply(lambda x: np.clip(x, 15.54, 44.71))\n",
    "# df['glucose'] = df['glucose'].apply(lambda x: np.clip(x, 40, 193))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94865ca7-15a8-4d81-8bdb-f76578438c60",
   "metadata": {},
   "source": [
    "### Изоляционный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466137a-afef-4282-bc51-04712ccdc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6476998-0b09-41f5-999d-8b5dacb40bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(n_estimators=100, n_jobs=-1, random_state=RANDOM_STATE, contamination=0.01) \n",
    "outliers = isolation_forest.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460a992-4507-47c5-a92a-5130c5e584c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[outliers==-1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117111dd-237f-4ac3-835b-a477753ad760",
   "metadata": {},
   "source": [
    "### Метод ближайших соседей для поиска аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8afb38-109e-4b96-b1c0-5d3173a15b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51151a-7503-410d-a46c-b552688aa26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = KNN(contamination=0.01, n_jobs=-1).fit_predict(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23271b6-c1f9-47aa-a0f8-5a1fd52abbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[outliers==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf0f83-2355-4010-815f-58024f0e1011",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86b953-ce65-4a5a-ab14-0308187324c8",
   "metadata": {},
   "source": [
    "## Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4a703-cbbc-495d-bb7d-437afdcf517a",
   "metadata": {},
   "source": [
    "Вернитесь к любому датасету с категориальными признаками, над которым мы работали (или скачайте новый с интернета). Предложите оптимальный метод кодирования признаков. Реализуйте его (достаточно `.fit_transform()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0290931-872b-48bb-8592-0de362aea385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83247e7f-f712-4845-9dc8-3476d3355490",
   "metadata": {},
   "source": [
    "## Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca16874-9021-4210-84a7-4b0e283c356c",
   "metadata": {},
   "source": [
    "Изучите эмбеддинги word2vec, обученные на википедии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c675529-8a0c-4185-987e-a28e05193152",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wiki = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc27873-d8b3-4f63-ada7-7cf7c05ed3cd",
   "metadata": {},
   "source": [
    "Вы можете также воспользоваться другой предобученной моделью или найти ее в интернете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f96215-74e1-4f01-92a7-7428f96d7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(api.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ab802-2cb2-4eb5-b6e7-21c69d068ab2",
   "metadata": {},
   "source": [
    "1. Спроецируйте эмбеддинги на плоскость. Возможно, вам придется перебрать несколько методов и гиперпараметров, чтобы получить более показательную картину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359af9a7-cd04-48ff-ae2d-dcc7c6a09d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324a833-9250-418b-a9be-9bcbdca5be43",
   "metadata": {},
   "source": [
    "2. Попробуйте выделить кластеры методом DBSCAN. Сделайте вывод по наблюдениям. Насколько вы довольны результатом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75eac2-a1a6-4ef1-ada1-fcbd53461139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f4fe2-cd4e-4634-9da9-f256a1cc0054",
   "metadata": {},
   "source": [
    "Сохранить визуализацию в файл можно с помощью написанной выше функции `plot_w2v(my_umap, w2v=model_wiki, save='file.png')`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b7c20-1013-458f-a2cb-55d507a5c303",
   "metadata": {},
   "source": [
    "## Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b013-9720-441e-b541-365d19c81b08",
   "metadata": {},
   "source": [
    "Попробуйте реализовать простую модель Word2Vec методом градиентного спуска. Обучающую выборку возьмите по своему усмотрению.\n",
    "\n",
    "Вам понадобится:\n",
    "\n",
    "- сделать one-hot encoding слов (`CountVectorizer` подойдет);\n",
    "- придумать, как легко определять, в одном ли контексте слова, и обучать на соответствующих парах;\n",
    "- возможно, собирать вектор контекста для слова;\n",
    "- обучить матрицу эмбеддингов: если слово встречается в контексте, вектора должны быть похожи, если нет - ортогональны.\n",
    "\n",
    "Изучите полученные эмбеддинги. Довольны ли вы результатом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371f433-e268-4f69-b4de-e0603c6c1a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
